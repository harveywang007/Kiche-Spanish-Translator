{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7084747d",
   "metadata": {},
   "source": [
    "# **Creating the Tokenizers**\n",
    "\n",
    "This notebook will explore the tokenization of the Kiche and Spanish sentences. The tokenizer used is the BERT Tokenizer, a subword tokenizer; this allows for the tokenization to better account for morphemes, as Kiche and Spanish are both highly synthetic languages.\n",
    "\n",
    "As noted, the following code in the notebook borrows heavily from the [Tensorflow tutorial for subword tokenization](https://www.tensorflow.org/text/guide/subwords_tokenizer). I have indicated which code is from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff976185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Tokenizer module from the Translation folder/package\n",
    "from Translation import Tokenizer as tok\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# This will be imported to generate the wordpiece vocabulary\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdc524",
   "metadata": {},
   "source": [
    "## Preprocessing and Exploration\n",
    "\n",
    "In this section, I load and preprocess the data, along with examining the first and last 5 examples for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90e0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'Bilingual Corpus.csv'\n",
    "source = 'Kiche'\n",
    "target = 'Spanish'\n",
    "\n",
    "all_ki, all_sp = tok.preclean(filepath, source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b0b0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weta’m chi ri utaqanik are k’aslemal ri maj uk...\n",
       "1    qeta’m chi ronojel ri tiktalik koq’ik jetaq ri...\n",
       "2    we k’u kape jun achi chirij ri sib’alaj k’o uc...\n",
       "3    pune’ ri e are’ man e q’ui taj , xa’ e kieb’ o...\n",
       "4    kraj k’u ne xekikamisaj juwinaq waqlajuj aj is...\n",
       "Name: Kiche, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 Kiche sentences\n",
    "all_ki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a9d08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37083    man xinwil ta pa ri tinimit ri’ jun templo rum...\n",
       "37084       i le jun chkech le e pareyib’ , le espanyolib’\n",
       "37085    xkik’am b’ik ri jesús pa ri ulew ub’i’nam gólg...\n",
       "37086      na kinb’eta pa le nimatijob’al rumal xinkosik .\n",
       "37087    e are’ k’ut ri kekanaj kan chiwe , kinya’ na n...\n",
       "Name: Kiche, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 5 Kiche sentences\n",
    "all_ki.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5603f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    y sé que su mandamiento es vida eterna ; así q...\n",
       "1    porque ya sabemos que todas las criaturas gime...\n",
       "2    mas si sobreviniendo otro más fuerte que él , ...\n",
       "3    siendo vosotros pocos hombres en número , y pe...\n",
       "4    los hombres de hai mataron a unos treinta y se...\n",
       "Name: Spanish, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 Spanish sentences\n",
    "all_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f22d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37083    y no vi en ella templo ; porque el señor dios ...\n",
       "37084    y lo otro es que para los sacerdores , los esp...\n",
       "37085    y le llevaron al lugar de gólgota , que declar...\n",
       "37086           no voy a la escuela porque estoy cansado .\n",
       "37087     “haré que aquellos de ustedes que sobrevivan ...\n",
       "Name: Spanish, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 5 Spanish sentences\n",
    "all_sp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dd142",
   "metadata": {},
   "source": [
    "## Generating the Wordpiece Vocabulary\n",
    "\n",
    "This section will generate a wordpiece vocabulary file for each language; these files store the \"morphemes\" that have been parsed by `bert_vocab.bert_vocab_from_dataset`. The `reserved_tokens` are the strings `\"[PAD]\"`, `\"[UNK]\"`, `\"[START]\"`, `\"[END]\"`.\n",
    "\n",
    "Much of the code is from the Tensorflow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4928d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arguments for the tokenizer are defined\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=tok.reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3935182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is converted into tensors\n",
    "kiche, spanish = tok.convert(all_ki, all_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72bfd773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where the magic happens\n",
    "# The wordpiece vocabulary is generated\n",
    "\n",
    "ki_vocab, sp_vocab = tok.generate_vocab(kiche, spanish, bert_vocab_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9d8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', \"'\", '(', ')', '*', ',']\n",
      "['taj', 'xeb', 'konojel', 'ob', 'chik', 'aretaq', 'je', 'wach', 'kab', 'anik']\n",
      "['##en', '##uk', 'amoreyib', 'eliy', 'kal', 'kuchomaj', 'mexa', 'onik', 'amonib', 'kanajinaq']\n",
      "['##h', '##v', '##¡', '##¿', '##–', '##—', '##‘', '##’', '##“', '##”']\n"
     ]
    }
   ],
   "source": [
    "# Prints out some vocab in Kiche\n",
    "print(ki_vocab[:10])\n",
    "print(ki_vocab[100:110])\n",
    "print(ki_vocab[1000:1010])\n",
    "print(ki_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b49d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', \"'\", '(', ')', ',', '-']\n",
      "['te', 'si', 'cuando', 'una', 'todo', 'entonces', 'hijo', 'asi', 'casa', 'hijos']\n",
      "['hallo', 'harina', 'hubiera', 'llamaba', 'menor', 'pos', 'ramas', 'sabes', 'subir', 'tomando']\n",
      "['##¡', '##«', '##»', '##¿', '##–', '##—', '##‘', '##’', '##“', '##”']\n"
     ]
    }
   ],
   "source": [
    "# Prints out some vocab in Spanish\n",
    "print(sp_vocab[:10])\n",
    "print(sp_vocab[100:110])\n",
    "print(sp_vocab[1000:1010])\n",
    "print(sp_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7cb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the Kiche vocab into a file\n",
    "tok.write_vocab_file('ki_vocab.txt', ki_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d1d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the Spanish vocab into a file\n",
    "tok.write_vocab_file('sp_vocab.txt', sp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddedba6d",
   "metadata": {},
   "source": [
    "## Building the Tokenizer\n",
    "\n",
    "In this section, the tokenizers will be built. The CustomTokenizer class defined in Tokenizer module will be used to build them. The cosntructor contains a BertTokenizer object that will generate a tokenizer when a vocabulary file is passed into it.\n",
    "\n",
    "Much of this code comes from the Tensorflow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ef11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the tokenizers for Kiche and Spanish\n",
    "ki_tok = tok.CustomTokenizer(tok.reserved_tokens, 'ki_vocab.txt')\n",
    "sp_tok = tok.CustomTokenizer(tok.reserved_tokens, 'sp_vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bab68",
   "metadata": {},
   "source": [
    "### Kiche Tokenization\n",
    "\n",
    "I will explore the tokenization of Kiche in this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f550b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'weta\\xe2\\x80\\x99m chi ri utaqanik are k\\xe2\\x80\\x99aslemal ri maj uk\\xe2\\x80\\x99isik . xaq jeri\\xe2\\x80\\x99 ronojel ri kinb\\xe2\\x80\\x99ij are ri\\xe2\\x80\\x99 ri ub\\xe2\\x80\\x99im ri tataxel chwe kinb\\xe2\\x80\\x99ij .', shape=(), dtype=string)\n",
      "tf.Tensor(b'qeta\\xe2\\x80\\x99m chi ronojel ri tiktalik koq\\xe2\\x80\\x99ik jetaq ri kub\\xe2\\x80\\x99an jun ixoq are kak\\xe2\\x80\\x99oji\\xe2\\x80\\x99 ral .', shape=(), dtype=string)\n",
      "tf.Tensor(b'we k\\xe2\\x80\\x99u kape jun achi chirij ri sib\\xe2\\x80\\x99alaj k\\xe2\\x80\\x99o uchuq\\xe2\\x80\\x99ab\\xe2\\x80\\x99 choch , kesax ri\\xe2\\x80\\x99 ri uch\\xe2\\x80\\x99eyab\\xe2\\x80\\x99al ri ku\\xe2\\x80\\x99l uk\\xe2\\x80\\x99u\\xe2\\x80\\x99x chirij , xuquje\\xe2\\x80\\x99 kelaq\\xe2\\x80\\x99ax b\\xe2\\x80\\x99ik ronojel ri jastaq rech .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# These are the untokenized sentences\n",
    "for ki_sentence in kiche.batch(3).take(1):\n",
    "    for ex in ki_sentence:\n",
    "        print(ex)\n",
    "        \n",
    "# \\xe2\\x80\\x99 represents ’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30be9cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 528, 57, 38, 64, 60, 1267, 70, 36, 57, 195, 60, 224, 164, 57, 1029, 11, 135, 153, 57, 99, 60, 147, 57, 67, 70, 60, 57, 60, 163, 57, 272, 60, 625, 138, 147, 57, 67, 11, 3]\n",
      "[2, 945, 57, 38, 64, 99, 60, 4876, 934, 57, 83, 338, 60, 111, 57, 78, 72, 226, 70, 157, 57, 198, 57, 375, 11, 3]\n",
      "[2, 76, 36, 57, 46, 277, 72, 158, 495, 60, 110, 57, 97, 36, 57, 40, 1360, 57, 77, 57, 476, 9, 1076, 60, 57, 60, 400, 57, 576, 229, 57, 65, 60, 304, 57, 37, 164, 57, 46, 57, 49, 495, 9, 66, 57, 2671, 57, 143, 27, 57, 83, 99, 60, 152, 79, 11, 3]\n"
     ]
    }
   ],
   "source": [
    "# The tokenized sentences\n",
    "ki_tokenized = ki_tok.tokenize(ki_sentence)\n",
    "\n",
    "for ex in ki_tokenized.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d15d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] weta \\xe2\\x80\\x99 m chi ri utaqanik are k \\xe2\\x80\\x99 aslemal ri maj uk \\xe2\\x80\\x99 isik . xaq jeri \\xe2\\x80\\x99 ronojel ri kinb \\xe2\\x80\\x99 ij are ri \\xe2\\x80\\x99 ri ub \\xe2\\x80\\x99 im ri tataxel chwe kinb \\xe2\\x80\\x99 ij . [END]',\n",
       "       b'[START] qeta \\xe2\\x80\\x99 m chi ronojel ri tiktalik koq \\xe2\\x80\\x99 ik jetaq ri kub \\xe2\\x80\\x99 an jun ixoq are kak \\xe2\\x80\\x99 oji \\xe2\\x80\\x99 ral . [END]',\n",
       "       b'[START] we k \\xe2\\x80\\x99 u kape jun achi chirij ri sib \\xe2\\x80\\x99 alaj k \\xe2\\x80\\x99 o uchuq \\xe2\\x80\\x99 ab \\xe2\\x80\\x99 choch , kesax ri \\xe2\\x80\\x99 ri uch \\xe2\\x80\\x99 eya ##b \\xe2\\x80\\x99 al ri ku \\xe2\\x80\\x99 l uk \\xe2\\x80\\x99 u \\xe2\\x80\\x99 x chirij , xuquje \\xe2\\x80\\x99 kelaq \\xe2\\x80\\x99 ax b \\xe2\\x80\\x99 ik ronojel ri jastaq rech . [END]'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokens are matched with their wordpieces\n",
    "\n",
    "# Lookup each token id in the vocabulary\n",
    "ki_txt_tokens = tf.gather(ki_vocab, ki_tokenized)\n",
    "# Join with spaces\n",
    "tf.strings.reduce_join(ki_txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ae8a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'weta \\xe2\\x80\\x99 m chi ri utaqanik are k \\xe2\\x80\\x99 aslemal ri maj uk \\xe2\\x80\\x99 isik . xaq jeri \\xe2\\x80\\x99 ronojel ri kinb \\xe2\\x80\\x99 ij are ri \\xe2\\x80\\x99 ri ub \\xe2\\x80\\x99 im ri tataxel chwe kinb \\xe2\\x80\\x99 ij . qeta \\xe2\\x80\\x99 m chi ronojel ri tiktalik koq \\xe2\\x80\\x99 ik jetaq ri kub \\xe2\\x80\\x99 an jun ixoq are kak \\xe2\\x80\\x99 oji \\xe2\\x80\\x99 ral . we k \\xe2\\x80\\x99 u kape jun achi chirij ri sib \\xe2\\x80\\x99 alaj k \\xe2\\x80\\x99 o uchuq \\xe2\\x80\\x99 ab \\xe2\\x80\\x99 choch , kesax ri \\xe2\\x80\\x99 ri uch \\xe2\\x80\\x99 eyab \\xe2\\x80\\x99 al ri ku \\xe2\\x80\\x99 l uk \\xe2\\x80\\x99 u \\xe2\\x80\\x99 x chirij , xuquje \\xe2\\x80\\x99 kelaq \\xe2\\x80\\x99 ax b \\xe2\\x80\\x99 ik ronojel ri jastaq rech .'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The detokenized, and reassembled, sentences\n",
    "ki_words = ki_tok.detokenize(ki_tokenized)\n",
    "tf.strings.reduce_join(ki_words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881d442",
   "metadata": {},
   "source": [
    "### Spanish Tokenization\n",
    "\n",
    "I will explore the tokenization of Spanish in this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f97ba505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'y s\\xc3\\xa9 que su mandamiento es vida eterna ; as\\xc3\\xad que , lo que yo hablo , como el padre me lo ha dicho , as\\xc3\\xad hablo .', shape=(), dtype=string)\n",
      "tf.Tensor(b'porque\\xc2\\xa0ya\\xc2\\xa0sabemos que todas las criaturas gimen , y est\\xc3\\xa1n de parto hasta ahora .', shape=(), dtype=string)\n",
      "tf.Tensor(b'mas si sobreviniendo otro m\\xc3\\xa1s fuerte que \\xc3\\xa9l , le venciere ,\\xc2\\xa0le\\xc2\\xa0toma todas sus armas en que confiaba , y reparte sus despojos .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# These are the untokenized sentences\n",
    "for sp_sentence in spanish.batch(3).take(1):\n",
    "    for ex in sp_sentence:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3949fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 49, 70, 65, 73, 659, 83, 229, 1020, 23, 107, 65, 8, 81, 65, 93, 387, 8, 86, 64, 134, 92, 81, 118, 288, 8, 107, 387, 10, 3]\n",
      "[2, 87, 194, 1326, 65, 139, 77, 4040, 31, 5134, 110, 8, 49, 216, 63, 1960, 123, 152, 10, 3]\n",
      "[2, 91, 101, 94, 6221, 4459, 213, 91, 504, 65, 64, 8, 88, 4730, 8, 88, 700, 139, 79, 1245, 68, 65, 3037, 252, 8, 49, 42, 174, 2368, 179, 79, 2705, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "# The tokenized sentences\n",
    "sp_tokenized = sp_tok.tokenize(sp_sentence)\n",
    "\n",
    "for ex in sp_tokenized.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9f866b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] y se que su mandamiento es vida eterna ; asi que , lo que yo hablo , como el padre me lo ha dicho , asi hablo . [END]',\n",
       "       b'[START] porque ya sabemos que todas las criaturas g ##ime ##n , y estan de parto hasta ahora . [END]',\n",
       "       b'[START] mas si sobre ##vi ##niendo otro mas fuerte que el , le venciere , le toma todas sus armas en que confia ##ba , y r ##e ##par ##te sus despojos . [END]'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokens are matched with their wordpieces\n",
    "\n",
    "# Lookup each token id in the vocabulary\n",
    "sp_txt_tokens = tf.gather(sp_vocab, sp_tokenized)\n",
    "# Join with spaces\n",
    "tf.strings.reduce_join(sp_txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ac92c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'y se que su mandamiento es vida eterna ; asi que , lo que yo hablo , como el padre me lo ha dicho , asi hablo . porque ya sabemos que todas las criaturas gimen , y estan de parto hasta ahora . mas si sobreviniendo otro mas fuerte que el , le venciere , le toma todas sus armas en que confiaba , y reparte sus despojos .'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The detokenized, and reassembled, sentences\n",
    "sp_words = sp_tok.detokenize(sp_tokenized)\n",
    "tf.strings.reduce_join(sp_words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168559e0",
   "metadata": {},
   "source": [
    "## Saving and Exporting the Model\n",
    "\n",
    "In this section, the tokenizers are saved and exported. The tokenizers are instantiated as a Module object to allow for consistent tokenization when reloaded.\n",
    "\n",
    "Again, the code comes from the Tensorflow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "175c7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kiche_spanish_tokens\\assets\n"
     ]
    }
   ],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.ki = ki_tok\n",
    "tokenizers.sp = sp_tok\n",
    "\n",
    "model_name = 'kiche_spanish_tokens'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d493b28",
   "metadata": {},
   "source": [
    "## Reloading and Testing the Tokenizers\n",
    "\n",
    "The saved and exported models are reloaded and tested.\n",
    "\n",
    "Again, the code comes from the Tensorflow tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba838184",
   "metadata": {},
   "source": [
    "### Kiche Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a60ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4985"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer is reloaded\n",
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.ki.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c57174f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 4865,  218, 4617, 1128, 4976, 1546,  715,    4,    3]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizes a phrase meaning \"Hello Tensorflow!\"\n",
    "tokens = reloaded_tokenizers.ki.tokenize(['Saqirik TensorFlow!'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4b0dc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'sedron', b'te', b'##nwaj', b'##or', b'##v', b'##it',\n",
       "  b'##w', b'!', b'[END]']]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokens are matched with their corresponding word pieces\n",
    "text_tokens = reloaded_tokenizers.ki.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b013ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saqirik tensorflow !\n"
     ]
    }
   ],
   "source": [
    "# The phrase is detokenized\n",
    "round_trip = reloaded_tokenizers.ki.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9407",
   "metadata": {},
   "source": [
    "### Spanish Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b19f0d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6678"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer is reloaded\n",
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.sp.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "518e6cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   32, 3609, 1606,  709,  147, 2431,  218, 6665,    4,    3]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizes a phrase meaning \"Hello Tensorflow!\"\n",
    "tokens = reloaded_tokenizers.sp.tokenize(['Hola TensorFlow!'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92599b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'h', b'##ola', b'tenido', b'amor', b'era', b'##gada',\n",
       "  b'gran', b'##w', b'!', b'[END]']]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokens are matched with their corresponding word pieces\n",
    "text_tokens = reloaded_tokenizers.sp.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc0cf257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola tensorflow !\n"
     ]
    }
   ],
   "source": [
    "# The phrase is detokenized\n",
    "round_trip = reloaded_tokenizers.sp.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b3d8b",
   "metadata": {},
   "source": [
    "## Zipping the Tokenizers for Future Use\n",
    "\n",
    "The tokenizers will be zipped for future use; they will be used in the transformer and seq2seq translators.\n",
    "\n",
    "Once again, the code comes from the Tensorflow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9202c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kiche_spanish_tokens/ (164 bytes security) (stored 0%)\n",
      "updating: kiche_spanish_tokens/assets/ (164 bytes security) (stored 0%)\n",
      "updating: kiche_spanish_tokens/assets/ki_vocab.txt (164 bytes security) (deflated 59%)\n",
      "updating: kiche_spanish_tokens/assets/sp_vocab.txt (164 bytes security) (deflated 60%)\n",
      "updating: kiche_spanish_tokens/saved_model.pb (164 bytes security) (deflated 91%)\n",
      "updating: kiche_spanish_tokens/variables/ (164 bytes security) (stored 0%)\n",
      "updating: kiche_spanish_tokens/variables/variables.data-00000-of-00001 (164 bytes security) (deflated 51%)\n",
      "updating: kiche_spanish_tokens/variables/variables.index (164 bytes security) (deflated 33%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r {model_name}.zip {model_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
